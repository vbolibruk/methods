Інтерполяція і екстраполяція:

Інтерполяція - це процес апроксимації функції або набору даних за допомогою поліномів чи інших функцій так, що ця апроксимація точно проходить через задані точки. Інтерполяція використовується для заповнення прогалин між відомими значеннями та отримання значень функції в проміжних точках.

Екстраполяція - це процес апроксимації функції або набору даних за допомогою поліномів чи інших функцій за межами відомих точок. Екстраполяція дозволяє прогнозувати значення функції поза інтервалом, який був використаний для апроксимації.

Види інтерполяції:

Лінійна інтерполяція: Використовується лінійний поліном для з'єднання двох точок.
Поліноміальна інтерполяція: Використовується поліном вищого ступеня для апроксимації.
Сплайн-інтерполяція: Використовується набір поліномів, які об'єднуються, щоб апроксимувати функцію.
Інтерполяція Ньютона і Лагранжа: Використовуються спеціальні поліноми Ньютона і Лагранжа для апроксимації функції.
Апроксимація і регресія:

Апроксимація - це процес побудови простої функції або моделі, яка наближає складні дані або функцію. Це може включати в себе використання поліномів, експоненційних функцій, лінійних моделей і т. д., але апроксимація не обов'язково повинна точно проходити через задані точки.

Регресія - це метод аналізу даних, в якому прагнуть знайти математичну функцію або модель, яка краще підходить для опису взаємозв'язку між залежними і незалежними змінними. Регресія використовується для прогнозування значень залежної змінної на основі незалежних змінних.

Прямолінійна, криволінійна, ортогональна, параболічна регресія:

Прямолінійна регресія - це вид регресії, в якому залежність між залежною і незалежною змінними описується лінійною функцією. Найвідоміший приклад - це лінійна регресія, де залежна змінна апроксимується лінійною функцією незалежної змінної.

Криволінійна регресія - це вид регресії, в якому залежність між залежною і незалежною змінними описується криволінійною функцією, а не лінійною. Це може включати в себе, наприклад, експоненційну функцію або логарифмічну функцію.

Ортогональна регресія - це метод регресії, в якому використовують ортогональні функції для опису взаємозв'язку між змінними. Це може включати в себе поліноми Чебишова, Лежандра і інші.

Параболічна регресія - це вид регресії, в якому залежність апроксимується параболічною функцією (квадратичною). Така регресія може бути використана, коли є підстави вважати, що залежні


